{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import lucid.modelzoo.vision_models as models\n",
    "from lucid.misc.io import show\n",
    "import lucid.optvis.objectives as objectives\n",
    "import lucid.optvis.param as param\n",
    "import lucid.optvis.render as render\n",
    "import lucid.optvis.transform as transform\n",
    "from lucid.misc.channel_reducer import ChannelReducer\n",
    "import sys\n",
    "\n",
    "from lucid.misc.io import show, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.InceptionResnet2()\n",
    "model.load_graphdef()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InceptionResnetV2/Mixed_5b/concat\n",
      "InceptionResnetV2/Repeat/block35_1/concat\n",
      "InceptionResnetV2/Repeat/block35_2/concat\n",
      "InceptionResnetV2/Repeat/block35_3/concat\n",
      "InceptionResnetV2/Repeat/block35_4/concat\n",
      "InceptionResnetV2/Repeat/block35_5/concat\n",
      "InceptionResnetV2/Repeat/block35_6/concat\n",
      "InceptionResnetV2/Repeat/block35_7/concat\n",
      "InceptionResnetV2/Repeat/block35_8/concat\n",
      "InceptionResnetV2/Repeat/block35_9/concat\n",
      "InceptionResnetV2/Repeat/block35_10/concat\n",
      "InceptionResnetV2/Mixed_6a/concat\n",
      "InceptionResnetV2/Repeat_1/block17_1/concat\n",
      "InceptionResnetV2/Repeat_1/block17_2/concat\n",
      "InceptionResnetV2/Repeat_1/block17_3/concat\n",
      "InceptionResnetV2/Repeat_1/block17_4/concat\n",
      "InceptionResnetV2/Repeat_1/block17_5/concat\n",
      "InceptionResnetV2/Repeat_1/block17_6/concat\n",
      "InceptionResnetV2/Repeat_1/block17_7/concat\n",
      "InceptionResnetV2/Repeat_1/block17_8/concat\n",
      "InceptionResnetV2/Repeat_1/block17_9/concat\n",
      "InceptionResnetV2/Repeat_1/block17_10/concat\n",
      "InceptionResnetV2/Repeat_1/block17_11/concat\n",
      "InceptionResnetV2/Repeat_1/block17_12/concat\n",
      "InceptionResnetV2/Repeat_1/block17_13/concat\n",
      "InceptionResnetV2/Repeat_1/block17_14/concat\n",
      "InceptionResnetV2/Repeat_1/block17_15/concat\n",
      "InceptionResnetV2/Repeat_1/block17_16/concat\n",
      "InceptionResnetV2/Repeat_1/block17_17/concat\n",
      "InceptionResnetV2/Repeat_1/block17_18/concat\n",
      "InceptionResnetV2/Repeat_1/block17_19/concat\n",
      "InceptionResnetV2/Repeat_1/block17_20/concat\n",
      "InceptionResnetV2/Mixed_7a/concat\n",
      "InceptionResnetV2/Repeat_2/block8_1/concat\n",
      "InceptionResnetV2/Repeat_2/block8_2/concat\n",
      "InceptionResnetV2/Repeat_2/block8_3/concat\n",
      "InceptionResnetV2/Repeat_2/block8_4/concat\n",
      "InceptionResnetV2/Repeat_2/block8_5/concat\n",
      "InceptionResnetV2/Repeat_2/block8_6/concat\n",
      "InceptionResnetV2/Repeat_2/block8_7/concat\n",
      "InceptionResnetV2/Repeat_2/block8_8/concat\n",
      "InceptionResnetV2/Repeat_2/block8_9/concat\n",
      "InceptionResnetV2/Block8/concat\n"
     ]
    }
   ],
   "source": [
    "for node in model.graph_def.node:\n",
    "  if 'Concat' in node.op:\n",
    "    print(node.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "modela = models.InceptionV1()\n",
    "modela.load_graphdef()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_activation_grid_less_naive(img, modela, layer=\"mixed4d\", W=42,\n",
    "                                 n_groups=6, subsample_factor=1, n_steps=256):\n",
    "  \n",
    "  # Get the activations\n",
    "  with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    t_input = tf.placeholder(\"float32\", [None, None, None, 3])\n",
    "    T = render.import_model(model, t_input, t_input)\n",
    "    acts = T(layer).eval({t_input: img[None]})[0]\n",
    "  acts_flat = acts.reshape([-1] + [acts.shape[2]])\n",
    "  N = acts_flat.shape[0]\n",
    "  \n",
    "  # The trick to avoiding \"decoherence\" is to recognize images that are\n",
    "  # for similar activation vectors and \n",
    "  if n_groups > 0:\n",
    "    reducer = ChannelReducer(n_groups, \"NMF\")\n",
    "    groups = reducer.fit_transform(acts_flat)\n",
    "    groups /= groups.max(0)\n",
    "  else:\n",
    "    groups = np.zeros([])\n",
    "    \n",
    "  print groups.shape\n",
    "\n",
    "  \n",
    "  # The key trick to increasing memory efficiency is random sampling.\n",
    "  # Even though we're visualizing lots of images, we only run a small\n",
    "  # subset through the network at once. In order to do this, we'll need\n",
    "  # to hold tensors in a tensorflow graph around the visualization process.\n",
    "  \n",
    "  with tf.Graph().as_default() as graph, tf.Session() as sess:\n",
    "    \n",
    "    \n",
    "    # Using the groups, create a paramaterization of images that\n",
    "    # partly shares paramters between the images for similar activation\n",
    "    # vectors. Each one still has a full set of unique parameters, and could\n",
    "    # optimize to any image. We're just making it easier to find solutions\n",
    "    # where things are the same.\n",
    "    group_imgs_raw = param.fft_image([n_groups, W, W, 3])\n",
    "    unique_imgs_raw = param.fft_image([N, W, W, 3])\n",
    "    opt_imgs = param.to_valid_rgb(tf.stack([\n",
    "            0.7*unique_imgs_raw[i] + \n",
    "            0.5*sum(groups[i, j] * group_imgs_raw[j] for j in range(n_groups))\n",
    "            for i in range(N) ]),\n",
    "            decorrelate=True)\n",
    "    \n",
    "    # Construct a random batch to optimize this step\n",
    "    batch_size = 64\n",
    "    rand_inds = tf.random_uniform([batch_size], 0, N, dtype=tf.int32)\n",
    "    pres_imgs = tf.gather(opt_imgs, rand_inds)\n",
    "    pres_acts = tf.gather(acts_flat, rand_inds)\n",
    "    obj = objectives.Objective.sum(\n",
    "      [objectives.direction(layer, pres_acts[n], batch=n)\n",
    "       for n in range(batch_size)\n",
    "      ])\n",
    "    \n",
    "    # Actually do the optimization...\n",
    "    T = render.make_vis_T(model, obj, param_f=pres_imgs)\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for i in range(n_steps):\n",
    "      T(\"vis_op\").run()\n",
    "      if (i+1) % (n_steps//2) == 0:\n",
    "        show(pres_imgs.eval()[::4])\n",
    "    \n",
    "    vis_imgs = opt_imgs.eval()\n",
    "    \n",
    "  # Combine the images and display the resulting grid\n",
    "  print \"\"\n",
    "  vis_imgs_ = vis_imgs.reshape(list(acts.shape[:2]) + [W, W, 3])\n",
    "  vis_imgs_cropped = vis_imgs_[:, :, 2:-2, 2:-2, :]\n",
    "  show(np.hstack(np.hstack(vis_imgs_cropped)))\n",
    "  return vis_imgs_cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"The name 'import/mixed4d:0' refers to a Tensor which does not exist. The operation, 'import/mixed4d', does not exist in the graph.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ce080508dc69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://storage.googleapis.com/lucid-static/building-blocks/examples/dog_cat.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrender_activation_grid_less_naive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodela\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m48\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-08993a57afa5>\u001b[0m in \u001b[0;36mrender_activation_grid_less_naive\u001b[0;34m(img, modela, layer, W, n_groups, subsample_factor, n_steps)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mt_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0macts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mt_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0macts_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0macts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macts_flat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/elebouder/TNSRVIS/lucid/lucid/optvis/render.pyc\u001b[0m in \u001b[0;36mT\u001b[0;34m(layer)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"input\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mt_image_raw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mt_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"import/%s:0\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/elebouder/tensorlucid/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mget_tensor_by_name\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3764\u001b[0m       raise TypeError(\"Tensor names are strings (or similar), not %s.\" %\n\u001b[1;32m   3765\u001b[0m                       type(name).__name__)\n\u001b[0;32m-> 3766\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_graph_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3768\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_tensor_by_tf_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/elebouder/tensorlucid/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3589\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3590\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3592\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/elebouder/tensorlucid/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3630\u001b[0m           raise KeyError(\"The name %s refers to a Tensor which does not \"\n\u001b[1;32m   3631\u001b[0m                          \u001b[0;34m\"exist. The operation, %s, does not exist in the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3632\u001b[0;31m                          \"graph.\" % (repr(name), repr(op_name)))\n\u001b[0m\u001b[1;32m   3633\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3634\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout_n\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"The name 'import/mixed4d:0' refers to a Tensor which does not exist. The operation, 'import/mixed4d', does not exist in the graph.\""
     ]
    }
   ],
   "source": [
    "img = load(\"https://storage.googleapis.com/lucid-static/building-blocks/examples/dog_cat.png\")\n",
    "_ = render_activation_grid_less_naive(img, modela, W=48, n_steps=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_activation_grid_less_naive(img, model, layer=\"InceptionResnetV2/Mixed_5b/concat\", W=42,\n",
    "                                 n_groups=6, subsample_factor=1, n_steps=256):\n",
    "  \n",
    "  # Get the activations\n",
    "  with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    t_input = tf.placeholder(\"float32\", [None, None, None, 3])\n",
    "    T = render.import_model(model, t_input, t_input)\n",
    "    acts = T(layer).eval({t_input: img[None]})[0]\n",
    "  acts_flat = acts.reshape([-1] + [acts.shape[2]])\n",
    "  N = acts_flat.shape[0]\n",
    "  print('1')\n",
    "  \n",
    "  # The trick to avoiding \"decoherence\" is to recognize images that are\n",
    "  # for similar activation vectors and \n",
    "  if n_groups > 0:\n",
    "    reducer = ChannelReducer(n_groups, \"NMF\")\n",
    "    groups = reducer.fit_transform(acts_flat)\n",
    "    groups /= groups.max(0)\n",
    "  else:\n",
    "    groups = np.zeros([])\n",
    "    \n",
    "  print groups.shape\n",
    "  print('2')\n",
    "\n",
    "  \n",
    "  # The key trick to increasing memory efficiency is random sampling.\n",
    "  # Even though we're visualizing lots of images, we only run a small\n",
    "  # subset through the network at once. In order to do this, we'll need\n",
    "  # to hold tensors in a tensorflow graph around the visualization process.\n",
    "  \n",
    "  with tf.Graph().as_default() as graph, tf.Session() as sess:\n",
    "    \n",
    "    \n",
    "    # Using the groups, create a paramaterization of images that\n",
    "    # partly shares paramters between the images for similar activation\n",
    "    # vectors. Each one still has a full set of unique parameters, and could\n",
    "    # optimize to any image. We're just making it easier to find solutions\n",
    "    # where things are the same.\n",
    "    group_imgs_raw = param.fft_image([n_groups, W, W, 3])\n",
    "    print '2_1'\n",
    "    unique_imgs_raw = param.fft_image([N, W, W, 3])\n",
    "    print '2_2'\n",
    "    opt_imgs = param.to_valid_rgb(tf.stack([\n",
    "            0.7*unique_imgs_raw[i] + \n",
    "            0.5*sum(groups[i, j] * group_imgs_raw[j] for j in range(n_groups))\n",
    "            for i in range(N) ]),\n",
    "            decorrelate=True)\n",
    "    print('3')\n",
    "    \n",
    "    # Construct a random batch to optimize this step\n",
    "    batch_size = 64\n",
    "    rand_inds = tf.random_uniform([batch_size], 0, N, dtype=tf.int32)\n",
    "    print '3_1'\n",
    "    pres_imgs = tf.gather(opt_imgs, rand_inds)\n",
    "    print '3_2'\n",
    "    pres_acts = tf.gather(acts_flat, rand_inds)\n",
    "    print '3_3'\n",
    "    obj = objectives.Objective.sum(\n",
    "      [objectives.direction(layer, pres_acts[n], batch=n)\n",
    "       for n in range(batch_size)\n",
    "      ])\n",
    "    print('4')\n",
    "    \n",
    "    # Actually do the optimization...\n",
    "    T = render.make_vis_T(model, obj, param_f=pres_imgs)\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('5')\n",
    "    \n",
    "    for i in range(n_steps):\n",
    "      T(\"vis_op\").run()\n",
    "      if (i+1) % (n_steps//2) == 0:\n",
    "        show(pres_imgs.eval()[::4])\n",
    "    print('6')\n",
    "    \n",
    "    vis_imgs = opt_imgs.eval()\n",
    "    \n",
    "  # Combine the images and display the resulting grid\n",
    "  print \"\"\n",
    "  vis_imgs_ = vis_imgs.reshape(list(acts.shape[:2]) + [W, W, 3])\n",
    "  vis_imgs_cropped = vis_imgs_[:, :, 2:-2, 2:-2, :]\n",
    "  show(np.hstack(np.hstack(vis_imgs_cropped)))\n",
    "  return vis_imgs_cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load(\"/home/elebouder/Data/ChestXray-NIHCC/test.png\")\n",
    "_ = render_activation_grid_less_naive(img, model, W=48, n_steps=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
